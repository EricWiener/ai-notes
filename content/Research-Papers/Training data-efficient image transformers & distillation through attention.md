---
tags: [flashcards]
source: https://ar5iv.labs.arxiv.org/html/2012.12877
aliases: [DeiT]
summary: uses [[Distillation]] to train a vision transformer with a CNN teacher using much less data than [[ViT An Image is Worth 16x16 Words Transformers for Image Recognition at Scale|ViT]].
---
