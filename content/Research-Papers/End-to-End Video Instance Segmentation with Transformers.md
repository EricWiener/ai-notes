---
tags: [flashcards]
source: https://arxiv.org/abs/2011.14503
aliases: [VisTR]
summary:
---

[Code](https://github.com/Epiphqny/VisTR)

### Overview
VisTR employs a non-auto-regressive variant of the Transformer to achieve parallel sequence generation (unlike the original Transformers that were auto-regressive). It predicts $n$ instances for $T$ frames using a total of $n \cdot T = N$ instance queries. It then tracks these queries over time using their index in the sequence. At training time it uses the Hungarian Algorithm to associate sequences of instances with their corresponding ground truth.

# Related Work
### Video Instance Segmentation
Previous work (like [[MaskTrack R-CNN|MaskTrack R-CNN]]) either rely on complex heuristic rules to associate the instances or require multiple steps to generate and optimize the masks iteratively. **In contrast, VisTR aims to build a simple and end-to-end trainable VIS framework.**

### Transformers
- Self-attention in Transformers makes it particularly good for modeling long-range dependencies.
- [[DETR|DETR]] builds an object detection systems based on Transformers, which largely simplifies the traditional detection pipeline, and achieves on par performances compared with highly-optimized CNN based detectors.
- [[ViT An Image is Worth 16x16 Words Transformers for Image Recognition at Scale|ViT]] introduces the Transformer to image recognition and models an image as a sequence of patches, which attains excellent results compared to state- of-the-art convolutional networks.
- No prior works have applied Transformers for video instance segmentation.
- Transformer's ability to model long-range dependencies make it an ideal candidate for learning temporal information across multiple frames for video understanding tasks.

# VisTR
![[vistr-architecture.png]]

The VisTR architecture has four main components: a CNN backbone to extract compact feature representations of multiple frames, an encoder-decoder Transformer to model the similarity of pixel-level and instance-level features, an instance sequence matching module for supervising the model, and an instance sequence segmentation module.

### CNN Backbone
- Given initial video clip ($x_{c l i p} \in \mathbb{R}^{T \times 3 \times H_0 \times W_0}$) with $T$ frames and resolution $H_0 \times W_0$, the CNN generates a lower-resolution activation for each frame.
- The per-frame features are then concatenated to form the clip level feature map $f_0 \in \mathbb{R}^{T \times C \times H \times W}$.

### Transformer Encoder
- A $1 \times 1$ convolution is applied to the feature map generated by the CNN ($f_0 \in \mathbb{R}^{T \times C \times H \times W}$). It reduces the channel dimension from $C$ to $d$ ($d < C$) resulting in a new feature map $f_1 \in \mathbb{R}^{T \times d \times H \times W}$.
- The spatial and temporal dimensions of $f_1$ are then flattened resulting into a 2D feature map of size $d \times(T \cdot H \cdot W)$ which provides $d$ input tokens.

**Temporal and spatial embeddings**:
- The Transformer architecture is permutation-invariant, while the segmentation task requires precise position information.
- They add positional encodings that contain information about the three dimensions (temporal, horizontal, and vertical).

They use a similar positional encoding setup as [[Attention is All You Need]]:
$$\mathrm{PE}(\operatorname{pos}, i)= \begin{cases}\sin \left(\operatorname{pos} \cdot \omega_k\right), & \text { for } i=2 k \\ \cos \left(\operatorname{pos} \cdot \omega_k\right), & \text { for } i=2 k+1\end{cases}$$ where $\omega_k=1 / 10000^{2 k / \frac{d}{3}}$. $\text{pos}$ is the location in a certain dimension (temporal, horizontal, and vertical dimensions are handled seperately). You end up with $d/3$ channels for each dimension and then concatenate them to form the final $d$ channel embedding.

### Transformer Decoder
![[end-to-end-video-instance-segmentation-with-transformers-20230111151543695.png]]

The decoder takes the encoded features (denotes "pixel features") from the encoder and a fixed number of input embeddings to query the "instance features" from the "pixel features." This is a similar setup to [[DETR|DETR]].

If the model decodes $n$ instances each frame, then for $T$ frames, there will be a total of $N = n \cdot T$ queries. The instance queries are learned by the model and have the same dimension with the pixel features    

**Notation:**
- The output of the encoder is $E$
- $N$ is the number of instance queries and is equal to the $n$ (instances per frame) * $T$ (number of frames).
- $O$ is the output of the decoder

### Instance Sequence Matching
VisTR predicts $n$ instances per frame so the number of instance sequencies (predictions for the same instance across different frames) is also $n$.

- $\hat{y}= \left\{\hat{y}_i\right\}_{i=1}^n$ are the predicted instance sequences ($n$ of them).
- $y$ is the ground truth set of instance sequences
- Assume $n$ (number of sequences) is larger than the ground truth set of instance sequences. Therefore, $y$ will be padded with $\emptyset$ to also be of size $n$.

To find a [[Bipartite Graph]] matching between the $\hat{y}$ and $y$, they use the [[Hungarian Algorithm]] to find a permutation of $n$ elements $\sigma \in S_n$ with the lowest cost:
$$\hat{\sigma}=\underset{\sigma \in S_n}{\arg \min } \sum_i^n \mathcal{L}_{\text {match }}\left(y_i, \hat{y}_{\sigma(i)}\right)$$
- $\mathcal{L}_{\text {match }}\left(y_i, \hat{y}_{\sigma(i)}\right)$ is a pair-wise matching cost between groujnd truth $y_i$ and the instance sequence prediction with index $\sigma(i)$.

**Bounding Box Prediction**
It is expensive to compute the mask similarity directly, so instead bounding boxes are predicted to perform the matching.

A 3-layer [[Linear|FFN]] with [[ReLU]] activation is applied to the object predictions $O$ of the Transformer decoder. Similarly to [[DETR#Prediction feed-forward network (FFN)]], the FFN predicts the normalized center coordinates, height, and width of the box. The linear layer predicts the class label with a softmax function. An additional "background" class is added to represent if no object is detected.

**Finding optimal assignment**
You will end up with $N = n \cdot T$ bounding box predictions for the object predictions sequence $O$. You can associate $n$ box sequences for each instance by their indices. This is referred to as `ins1 seq ... ins4 seq` in the diagram below. **At inference time, you can directly use the indices of the boxes to find what instance sequence they belong to.**

![[vistr-instance-sequence-matching.png]]

> [!NOTE] Tracking instances across frames
> They arrange the output ground truth so you only compare entire predicted instance sequences (the same token index is used for each frame) against ground truth sequences. This forces the model to learn to predict sequences in the correct order. 

**Matching Loss**
The matching loss takes both the class predictions and similarity of predicted/ground truth boxes into account.

Each element $i$ of the ground truth set $y$ is represented as:
$$y_i=\left\{\left(c_i, c_i \ldots, c_i\right),\left(b_{i, 0}, b_{i, 1} \ldots, b_{i, T}\right)\right\}$$
- $c_i$ is the class label for instance $i$ and is the same from time = 0 to time = $T$. It can be $\emptyset$.
- The bounding box position can change over time (vs. the class label) and the different bounding boxes are denoted $b_{i, t} \in [0, 1]^4$ where $i$ is the instance and $t$ is the timestep. They are vectors that define ground truth box center coordinates, height, and width relative to the image dimensions.

The predicted sequence instance index may differ from the ground truth index. The index of the predicted sequence instance that best matches the ground truth instance $i$ is denoted $\sigma(i)$.

The predicted probabilities for instance $\sigma(i)$ are denoted:
$$\hat{p}_{(\sigma(i))}\left(c_i\right)=\left\{\hat{p}_{(\sigma(i), 0)}\left(c_i\right) \ldots, \hat{p}_{(\sigma(i), T)}\left(c_i\right)\right\}$$
where $c_i$ is the correct class label for this instance sequence (based on the optimal matching). Note that the predicted confidence for the class can vary over time. For example, $\hat{p}_{(\sigma(i), 0)}\left(c_i\right)$ is the confidence for class $c_i$ for the predicted sequence with index $\sigma(i)$ at time $0$.

The predicted box sequence for instance $\sigma(i)$ are denoted as:
$$\hat{b}_{\sigma(i)}=\left\{\hat{b}_{(\sigma(i), 0)}, \hat{b}_{(\sigma(i), 1)} \ldots, \hat{b}_{(\sigma(i), T)}\right\}$$
For example, $\hat{b}_{(\sigma(i), 0)}$ is the predicted box at time 0.

The matching cost can be defined as:
$$\mathcal{L}_{\text {match }}\left(y_i, \hat{y}_{\sigma(i)}\right)=-\hat{p}_{\sigma(i)}\left(c_i\right)+\mathcal{L}_{\text {box }}\left(b_i, \hat{b}_{\sigma(i)}\right)$$
where $c_i \neq \emptyset$ (no cost is assigned when the ground truth class is background). This matching cost will be lower when the confidence for the correct class is higher and the bounding boxes match well. Using the above criterion, you can find a one-to-one matching based on the Hungarian algorithm.

**Hungarian Loss**
Once you find the optimal assignment, you can compute the loss function, the Hungarian loss, for all pairs matched in the previous step. This is a combination of negative log-likelihood for class prediction, a box loss, and a mask loss for the instance sequences:
$$\mathcal{L}_{\text {Hung }}(y, \hat{y})=\sum_{i=1}^N\left[\left(-\log \hat{p}_{\hat{\sigma}(i)}\left(c_i\right)\right)+\mathcal{L}_{\text {box }}\left(b_i, \hat{b}_{\hat{\sigma}}(i)\right)\right. \left.+\mathcal{L}_{\text {mask }}\left(m_i, \hat{m}_{\hat{\sigma}}(i)\right)\right]$$

where $c_i \neq \emptyset$ and $\hat{\sigma}$ is the optimal assignment computed with $\hat{\sigma}=\arg \min \sum_{\sigma \in S_n}^n \mathcal{L}_i \operatorname{match}\left(y_i, \hat{y}_{\sigma(i)}\right)$.

The Hungarian loss is used to train the whole framework.

**Bounding Box Loss**
They used a modified form of the loss used in [[DETR|DETR]] that supports multiple frames.
$$\mathcal{L}_{\mathrm{box}}\left(b_i, \hat{b}_{\sigma(i)}\right)=\frac{1}{T} \sum_{t=1}^T\left[\lambda_{\mathrm{iou}} \cdot \mathcal{L}_{\mathrm{iou}}\left(b_{i, t}, \hat{b}_{\sigma(i), t}\right)\right. + \left.+\lambda_{\mathrm{L} 1}\left\|b_{i, t}-\hat{b}_{\sigma(i), t}\right\|_1\right]$$
the two losses (L1 loss and generalized IOU loss) are normalized by the number of GT instances in the batch.

### Instance Sequence Segmentation
![[instance-sequence-segmentation.png]]
This module aims to predict the mask sequence for each instance.

To the best of my understanding, for each frame, they take the corresponding object predictions $O$ (from the transformer decoder) and the encoder output $E$ and form these into a sequence. They then pass this sequence to a self-attention module. You then get out an output where all the object predictions have associated with each other and the image features. This is done seperately for each frame (you don't mix info between frames).

You then concatenate the attention maps with the backbone features $B$ and encoder output $E$.

They use a 3D conv to refine the final mask predictions.

> [!question] This section of the paper was quite confusing and I'm not sure I got it right.
