---
tags: [flashcards]
source: https://cloud.google.com/translate/automl/docs/evaluate
summary: metric for automatically evaluating machine-translated text
---
BLEU (BiLingual Evaluation Understudy) is a metric for automatically evaluating machine-translated text. The BLEU score is **a number between zero and one that measures the ==similarity of the machine-translated text to a set of high quality reference translations==.
<!--SR:!2024-08-24,376,270-->