---
tags: [flashcards]
source: https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/articles/iclr-2022-spotlight-demystifying-local-attention-and-dynamic-depth-wise-convolution-2/
summary: local attention was introduced by [[Swin Transformer|Swin]] and is the idea that some inputs and outputs are not connected when computing attention. 
---




