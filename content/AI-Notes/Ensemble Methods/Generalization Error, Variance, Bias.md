---
tags: [flashcards]
source:
summary: generalization error is the error you get when applying a model to data it has not seen before. 
---

> [!NOTE] Trick to remember variance vs. bias
> Bias is like **Being Ignorant**. The model is just too simple for the problem. **Variance** sounds like **variation**, which can remind you that the model is **varying** a lot based on small changes in the data. This is a sign that the model is overfitting the data.

**Generalization error** is the error that a machine learning model makes on data that it has not seen before. It is the difference between the model's predictions on the training data and its predictions on the test data.

The goal of machine learning is to build models that generalize well. This means that the model should be able to make accurate predictions on new data, even if it is different from the data that the model was trained on.

Generalization error comes from:
- ==bias== (structural error - like using linear regression when you should use neural network)
- ==variance== (estimation error - overfitting on training error)
- ==noise== (crap in your dataset).
<!--SR:!2024-12-25,451,310!2024-12-28,454,310!2024-07-02,274,319-->

Both bias and variance are sources of error in machine learning models, and they can have a significant impact on the accuracy of the model.

**Bias** is the error that is introduced by the model's assumptions. It is the difference between the model's predictions and the true value. A model with high bias is too simple and does not capture the complexity of the data. As a result, it will make inaccurate predictions.

**Variance** is the error that is introduced by the model's training data. It is the difference between the model's predictions on the training data and its predictions on new data. A model with high variance is too complex and is sensitive to small changes in the training data. As a result, it will make inaccurate predictions on new data.

The **bias-variance trade-off** is the challenge of finding a model that has both low bias and low variance. It is a difficult problem, and there is no single solution that works for all problems. However, there are a number of techniques that can be used to reduce bias and variance, such as:

- **Regularization:** This is a technique that penalizes the model for being too complex. This can help to reduce variance.
- **Ensemble learning:** This is a technique that combines the predictions of multiple models. This can help to reduce bias.
- **Data augmentation:** This is a technique that artificially creates new data by adding noise or occlusion to existing data. This can help the model to learn to deal with these problems. It **can help with bias** by providing the model with more data so it can better understand the underlying patterns in the data and to make more accurate predictions. It **can also help with variance** by making the model more robust to small changes in the training data. This is because the model has seen more variations of the data, so it is less likely to be thrown off by a small change in the test data.

# Original Notes
### Generalization Error
We want to minimize generalization error. This is the error you get when applying your model to unseen data. Error comes from:

- Bias (structural error)
- Variance (estimation error)
- Noise

**This is formalized as:**
- Expected loss with respect to a randomly drawn datapoint and label from your dataset.
- However, the loss will change depending on what model you used. We need to account for all possible models and find the expected loss across all datasets.
- This is the expected loss from a **randomly drawn datapoint from a random dataset.**

### Noise
- Didn’t measure features that affected something
- Noisy labels (clicked ‘like’ and meant ‘dislike’)
- Sensor error

### Variance: estimation error
Measures how much the models for individual datasets vary around the expected model. If you trained one model on a dataset generated by some distribution, then generated a new dataset from the distribution, how much would your new model change.

As the number of data points increases, the affect changing a few of them has decreases. This means the variance decreases as the number of data points increases.

**Example:**
- Modeling regression with a high degree polynomial and a small dataset
- A high variance model.

### Bias: structural error
Measures extent to which average prediction over all datasets differs from desired function

**Example:**
- Suppose you are modeling regression with a linear function but the true function is non-linear
- This is a high bias model
- Increased n doesn’t help

### Variance-Bias Tradeoff

[https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229](https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229) 

[https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/](https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/) 

[https://elitedatascience.com/bias-variance-tradeoff](https://elitedatascience.com/bias-variance-tradeoff) 

![[AI-Notes/Video/Untitled]]

Variance exists when the model is very adaptable and changes with small changes in data. Bias occurs when your model is too restrictive and doesn’t fit the data well.

![[AI-Notes/Video/Untitled]]

As the model complexity increases, training error decreases, but validation error increases

Highly complex model (200 degree poly) → high **variance** Low complexity (linear) → high **bias** because you get high training error